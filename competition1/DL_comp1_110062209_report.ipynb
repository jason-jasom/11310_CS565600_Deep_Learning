{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqpCGYoqWZ93"
      },
      "source": [
        "姓名:簡晟棋\n",
        "\n",
        "學號:110062209"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V9zT3r-y1qhT"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOf3TsieCzrt",
        "outputId": "2915e259-bcfd-4bb8-d1b3-35e5d517304a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1CueEO9KKA5QGFB2uijp_mPsw9_jB3uFU\n",
            "From (redirected): https://drive.google.com/uc?id=1CueEO9KKA5QGFB2uijp_mPsw9_jB3uFU&confirm=t&uuid=045b3621-9534-4896-8bce-6b93d24c2424\n",
            "To: /content/dataset.zip\n",
            "100% 97.2M/97.2M [00:02<00:00, 42.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1CueEO9KKA5QGFB2uijp_mPsw9_jB3uFU\n",
        "!unzip -qq -u dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m2uKxSlWZ97"
      },
      "source": [
        "我曾經試過對content內容用countvectorizer與tfidfvectorizer,但結果都不盡理想。因此，我認為比起內容，能見度或許更重要，於是只提取了topic、發布時間(詳細到小時)這兩種features:\n",
        "\n",
        "發布時間:影響新聞能見度\n",
        "\n",
        "topic:比content更簡潔，比channel更詳細。除了能夠當作內容key word外，還能代替channel影響新聞能見度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jTHTGIxGk9s",
        "outputId": "88af6809-cb39-454a-a465-81285bac9bb8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "day_to_num = {'mon':1,'tue':2,'wed':3,'thu':4,'fri':5,'sat':6,'sun':7}\n",
        "month_to_num = {'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12}\n",
        "\n",
        "def preprocessor(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "\n",
        "    a_list = soup.find('footer').find_all('a')\n",
        "    topic_list = [a.get_text().strip().lower() for a in a_list]\n",
        "    topic = ' '.join([re.sub('\\s+', '_', t) for t in topic_list])\n",
        "\n",
        "    time_tag = soup.find('time')\n",
        "    if time_tag and 'datetime' in time_tag.attrs:\n",
        "        date_time = time_tag['datetime']\n",
        "    else:\n",
        "        date_time = 'Thu, 10 Oct 2024 00:00:00'\n",
        "    time_obj = re.search('([\\w]+),\\s+([\\d]+)\\s+([\\w]+)\\s+([\\d]+)\\s+([\\d]+)', date_time)\n",
        "    day, date, month, year, hour = time_obj.groups()\n",
        "    day, month = day_to_num[day.lower()], month_to_num[month.lower()]\n",
        "\n",
        "    return topic, day, date, month, year, hour\n",
        "\n",
        "def tokenizer_lem_nostop(text):\n",
        "    wnl = WordNetLemmatizer()\n",
        "    return [wnl.lemmatize(w) for w in re.split('\\s+', text.strip()) if w not in stop and re.match('[a-zA-Z]+', w)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HJ0rXtIzUNvQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('dataset/train.csv')\n",
        "test = pd.read_csv('dataset/test.csv')\n",
        "feature_train = []\n",
        "feature_test = []\n",
        "for text in train['Page content']:\n",
        "    feature_train.append(preprocessor(text))\n",
        "for text in test['Page content']:\n",
        "    feature_test.append(preprocessor(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qNh5cFzAZfTX"
      },
      "outputs": [],
      "source": [
        "columns=['Topic', 'Day', 'Date', 'Month', 'Year', 'Hour']\n",
        "df_train = pd.DataFrame(\n",
        "    feature_train,\n",
        "    columns=columns\n",
        ")\n",
        "df_test = pd.DataFrame(\n",
        "    feature_test,\n",
        "    columns=columns\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2PhB1UYWZ99"
      },
      "source": [
        "對於topic,我嘗試tfidfvectorizer與countvectorizer兩種方式轉換"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OXJDM2xpWRXQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
        "\n",
        "trans_count = ColumnTransformer([('Topic', CountVectorizer(tokenizer=tokenizer_lem_nostop),'Topic')],remainder='passthrough')\n",
        "trans_tfidf = ColumnTransformer([('Topic', TfidfVectorizer(tokenizer=tokenizer_lem_nostop),'Topic')],remainder='passthrough')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Tn_KawUzBkio"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y_train_raw = (train['Popularity'].values==1).astype(int)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(df_train,y_train_raw,test_size=0.2,random_state=0)\n",
        "X_train = pd.DataFrame(X_train,columns=columns)\n",
        "X_valid = pd.DataFrame(X_valid,columns=columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj85tLyKWZ9-"
      },
      "source": [
        "而在model的部分,我試了LightBoost,XGBoost,RandomForest等classifier。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fPaYBrJ5Ijvv"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGIIyWfVXQfa",
        "outputId": "8e3071d9-353f-41ec-941b-fe02eac6fb15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score: 0.600 (+/-0.009)\n",
            "train score:0.6858100659944703\n",
            "valid score:0.591644449980987\n"
          ]
        }
      ],
      "source": [
        "lgbm_tf = Pipeline([('vect', trans_tfidf),('clf', LGBMClassifier(learning_rate=0.01,n_estimators=300,verbose=0))])\n",
        "\n",
        "scores = cross_val_score(estimator=lgbm_tf, X=df_train, y=y_train_raw, cv=5, scoring='roc_auc')\n",
        "print('score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))\n",
        "\n",
        "lgbm_tf.fit(X_train, y_train)\n",
        "train_score = lgbm_tf.predict_proba(X_train)[:,1]\n",
        "print(f'train score:{roc_auc_score(y_train,train_score)}')\n",
        "valid_score = lgbm_tf.predict_proba(X_valid)[:,1]\n",
        "print(f'valid score:{roc_auc_score(y_valid,valid_score)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m2CNF1fHgIc",
        "outputId": "30c17e95-9ba7-461a-bba7-7baee5c37f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score: 0.604 (+/-0.007)\n",
            "train score:0.6697293570976751\n",
            "valid score:0.5962876435099319\n"
          ]
        }
      ],
      "source": [
        "lgbm_ct = Pipeline([('vect', trans_count),('clf', LGBMClassifier(learning_rate=0.01,n_estimators=300,verbose=0))])\n",
        "\n",
        "scores = cross_val_score(estimator=lgbm_ct, X=df_train, y=y_train_raw, cv=5, scoring='roc_auc')\n",
        "print('score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))\n",
        "\n",
        "lgbm_ct.fit(X_train, y_train)\n",
        "train_score = lgbm_ct.predict_proba(X_train)[:,1]\n",
        "print(f'train score:{roc_auc_score(y_train,train_score)}')\n",
        "valid_score = lgbm_ct.predict_proba(X_valid)[:,1]\n",
        "print(f'valid score:{roc_auc_score(y_valid,valid_score)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjdi9t7SHp3y",
        "outputId": "0d3c43db-8786-4fbf-afc3-fa303c8a0faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score: 0.595 (+/-0.009)\n",
            "train score:0.682518053502863\n",
            "valid score:0.5867981723477438\n"
          ]
        }
      ],
      "source": [
        "xgb_tf = Pipeline([('vect', trans_tfidf),('clf', XGBClassifier(eta=0.01,n_estimators=300,verbose=0))])\n",
        "\n",
        "scores = cross_val_score(estimator=xgb_tf, X=df_train, y=y_train_raw, cv=5, scoring='roc_auc')\n",
        "print('score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))\n",
        "\n",
        "xgb_tf.fit(X_train, y_train)\n",
        "train_score = xgb_tf.predict_proba(X_train)[:,1]\n",
        "print(f'train score:{roc_auc_score(y_train,train_score)}')\n",
        "valid_score = xgb_tf.predict_proba(X_valid)[:,1]\n",
        "print(f'valid score:{roc_auc_score(y_valid,valid_score)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_9olQedH2A8",
        "outputId": "8d0ce2b4-0bd4-4ee2-9450-634025c46dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score: 0.599 (+/-0.007)\n",
            "train score:0.6672210010720567\n",
            "valid score:0.5873803380565218\n"
          ]
        }
      ],
      "source": [
        "xgb_ct = Pipeline([('vect', trans_count),('clf', XGBClassifier(eta=0.01,n_estimators=300,verbose=0))])\n",
        "\n",
        "scores = cross_val_score(estimator=xgb_ct, X=df_train, y=y_train_raw, cv=5, scoring='roc_auc')\n",
        "print('score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))\n",
        "\n",
        "xgb_ct.fit(X_train, y_train)\n",
        "train_score = xgb_ct.predict_proba(X_train)[:,1]\n",
        "print(f'train score:{roc_auc_score(y_train,train_score)}')\n",
        "valid_score = xgb_ct.predict_proba(X_valid)[:,1]\n",
        "print(f'valid score:{roc_auc_score(y_valid,valid_score)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkjZV33OIXWS",
        "outputId": "51c6458b-d30d-4234-b5a5-a8fbfb303188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score: 0.579 (+/-0.010)\n",
            "train score:0.9999602872230277\n",
            "valid score:0.5865560285751897\n"
          ]
        }
      ],
      "source": [
        "rf_tf = Pipeline([('vect', trans_tfidf),('clf', RandomForestClassifier(n_estimators=300,verbose=0))])\n",
        "\n",
        "scores = cross_val_score(estimator=rf_tf, X=df_train, y=y_train_raw, cv=5, scoring='roc_auc')\n",
        "print('score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))\n",
        "rf_tf.fit(X_train, y_train)\n",
        "train_score = rf_tf.predict_proba(X_train)[:,1]\n",
        "print(f'train score:{roc_auc_score(y_train,train_score)}')\n",
        "valid_score = rf_tf.predict_proba(X_valid)[:,1]\n",
        "print(f'valid score:{roc_auc_score(y_valid,valid_score)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us_aBfJMIDQ0",
        "outputId": "e54a95c5-2243-4132-f169-36bf1026b4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score: 0.578 (+/-0.011)\n",
            "train score:0.9999567855607812\n",
            "valid score:0.5856666850733445\n"
          ]
        }
      ],
      "source": [
        "rf_ct = Pipeline([('vect', trans_count),('clf', RandomForestClassifier(n_estimators=300,verbose=0))])\n",
        "\n",
        "scores = cross_val_score(estimator=rf_ct, X=df_train, y=y_train_raw, cv=5, scoring='roc_auc')\n",
        "print('score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))\n",
        "rf_ct.fit(X_train, y_train)\n",
        "train_score = rf_ct.predict_proba(X_train)[:,1]\n",
        "print(f'train score:{roc_auc_score(y_train,train_score)}')\n",
        "valid_score = rf_ct.predict_proba(X_valid)[:,1]\n",
        "print(f'valid score:{roc_auc_score(y_valid,valid_score)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3u3SXKlWZ9_"
      },
      "source": [
        "最終，我選擇用countvectorizer轉換topic，並用LightBoost當主要的classifier，因為該組合cross validation平均分數最高"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cFkFAAeWbRYk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"output/\") : os.mkdir(\"output/\")\n",
        "test_id = test['Id']\n",
        "Y_test = lgbm_ct.predict_proba(df_test)[:,1]\n",
        "data = {\n",
        "    'Id': test_id,\n",
        "    'Popularity': Y_test\n",
        "}\n",
        "pd.DataFrame(data).to_csv('output/result.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVJkgIr8Ej0W"
      },
      "source": [
        "從上面實驗可以得知比起內容是什麼,topic與發佈時間這些影響能見度的因素才是決定新聞popularity的關鍵。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
