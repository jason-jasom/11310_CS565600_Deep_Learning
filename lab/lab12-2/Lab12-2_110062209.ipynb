{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.15.1"
      ],
      "metadata": {
        "id": "5MQBIL8NDtAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 19yR6-qVYJ6gQ-3HuXEOxjf-ZeaWJJruh\n",
        "!unzip -qq -u words_captcha.zip"
      ],
      "metadata": {
        "id": "VdKld28RmyEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a3bdd1-b73e-46c3-9215-5fc31e7de112"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=19yR6-qVYJ6gQ-3HuXEOxjf-ZeaWJJruh\n",
            "From (redirected): https://drive.google.com/uc?id=19yR6-qVYJ6gQ-3HuXEOxjf-ZeaWJJruh&confirm=t&uuid=a2db0f94-5974-4605-8fc8-d054bc2f456f\n",
            "To: /content/words_captcha.zip\n",
            "100% 4.57G/4.57G [00:51<00:00, 89.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_fU7p13OW85X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de987b94-1abb-49d5-985a-a5464d1534fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle\n",
        "IMAGE_SIZE = 224"
      ],
      "metadata": {
        "id": "PJCCSAl6XFbl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store captions and image names in vectors\n",
        "train_captions = []\n",
        "img_name_vector = []\n",
        "\n",
        "PATH = './words_captcha/'\n",
        "with open(PATH + 'spec_train_val.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        image_name, caption = line.split(' ')\n",
        "        full_image_path = PATH + image_name + '.png'\n",
        "        caption = '<start> ' + ' '.join(caption) + ' <end>'\n",
        "        img_name_vector.append(full_image_path)\n",
        "        train_captions.append(caption)"
      ],
      "metadata": {
        "id": "YPDELcmYYazL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "image_features_extract_model使用yolo的extractor部分"
      ],
      "metadata": {
        "id": "fVo5iVuV-3Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "MRzuW2Vd-GUV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_leaky_relu(inputs, filters, size, stride):\n",
        "    x = layers.Conv2D(filters, size, stride, padding=\"same\",\n",
        "                      kernel_initializer=tf.keras.initializers.TruncatedNormal())(inputs)\n",
        "    x = layers.LeakyReLU(0.1)(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "YqdiuTAq-E2-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_inputs = keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "x = conv_leaky_relu(img_inputs, 64, 7, 2)\n",
        "x = layers.MaxPool2D()(x)\n",
        "x = conv_leaky_relu(x, 192, 3, 1)\n",
        "x = layers.MaxPool2D()(x)\n",
        "x = conv_leaky_relu(x, 128, 1, 1)\n",
        "x = conv_leaky_relu(x, 256, 3, 1)\n",
        "x = conv_leaky_relu(x, 256, 1, 1)\n",
        "x = conv_leaky_relu(x, 512, 3, 1)\n",
        "x = layers.MaxPool2D()(x)\n",
        "x = conv_leaky_relu(x, 256, 1, 1)\n",
        "x = conv_leaky_relu(x, 512, 3, 1)\n",
        "x = conv_leaky_relu(x, 256, 1, 1)\n",
        "x = conv_leaky_relu(x, 512, 3, 1)\n",
        "x = conv_leaky_relu(x, 256, 1, 1)\n",
        "x = conv_leaky_relu(x, 512, 3, 1)\n",
        "x = conv_leaky_relu(x, 256, 1, 1)\n",
        "x = conv_leaky_relu(x, 512, 3, 1)\n",
        "x = conv_leaky_relu(x, 512, 1, 1)\n",
        "x = conv_leaky_relu(x, 1024, 3, 1)\n",
        "outputs = layers.MaxPool2D()(x)\n",
        "image_features_extract_model = keras.Model(inputs=img_inputs, outputs=outputs, name=\"YOLO_feature_extractor\")"
      ],
      "metadata": {
        "id": "v6k1sG0j-Ilu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "# Pad each vector to the max_length of the captions\n",
        "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
        "# Calculates the max_length, which is used to store the attention weights\n",
        "max_length = calc_max_length(train_seqs)"
      ],
      "metadata": {
        "id": "mPHaweH8YyA6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name_train = img_name_vector[:100000]\n",
        "cap_train = cap_vector[:100000]\n",
        "img_name_val = img_name_vector[100000:]\n",
        "cap_val = cap_vector[100000:]\n",
        "\n"
      ],
      "metadata": {
        "id": "Hom7XeXG9EZu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name_test = [PATH + f'a{i}.png' for i in range(120000,140000)]"
      ],
      "metadata": {
        "id": "tJr2uR0tjXr8"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change these parameters according to your system's configuration\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "BUFFER_SIZE = 5000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "LEARNING_RATE = 1e-4"
      ],
      "metadata": {
        "id": "REGEMWCyZUzQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "把Parallel mapping、prefetch這些加速方法用上"
      ],
      "metadata": {
        "id": "ZT5MrIAz_C-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_func(img_name, cap):\n",
        "    img = tf.io.read_file(img_name)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    img = img / 255 * 2 - 1\n",
        "    return img, cap\n",
        "\n",
        "def map_test(img_name):\n",
        "    img = tf.io.read_file(img_name)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    img = img / 255 * 2 - 1\n",
        "    return img"
      ],
      "metadata": {
        "id": "vscocpuhZWeW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\\\n",
        "        .shuffle(BUFFER_SIZE)\\\n",
        "        .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
        "        .batch(BATCH_SIZE)\\\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_valid = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val))\\\n",
        "        .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
        "        .batch(BATCH_SIZE)\\\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n"
      ],
      "metadata": {
        "id": "cyF_ljjdZZ5R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices(img_name_test)\\\n",
        "        .map(map_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
        "        .batch(BATCH_SIZE)\\\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Ex5j7fy6jZlU"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # score shape == (batch_size, 64, hidden_size)\n",
        "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "        # attention_weights shape == (batch_size, 64, 1)\n",
        "        # you get 1 at the last axis because you are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "c3wCUuGwZc5p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eRLfL8eEZeiJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # defining attention as a separate model\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # shape == (batch_size, max_length, hidden_size)\n",
        "        x = self.fc1(output)\n",
        "\n",
        "        # x shape == (batch_size * max_length, hidden_size)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ],
      "metadata": {
        "id": "dkvHzu89ZiEu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "metadata": {
        "id": "AAJErFOEZoO2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "W8MDYqwEZqxT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(image_features_extract_model = image_features_extract_model,\n",
        "                            encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
      ],
      "metadata": {
        "id": "me1RSL_GZvNR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
      ],
      "metadata": {
        "id": "yOQE_OftZx-N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding this in a separate cell because if you run the training cell\n",
        "# many times, the loss_plot array will be reset\n",
        "loss_plot = []"
      ],
      "metadata": {
        "id": "bwoLQmGoZ0tH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "\n",
        "    # initializing the hidden state for each batch\n",
        "    # because the captions are not related from image to image\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = image_features_extract_model(img_tensor)\n",
        "        features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
        "        features = encoder(features)\n",
        "\n",
        "        for i in range(1, target.shape[1]):\n",
        "            # passing the features through the decoder\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables + image_features_extract_model.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss"
      ],
      "metadata": {
        "id": "dqfQiaVMZ6RS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "start = time.time()\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in tqdm(enumerate(dataset_train), total=num_steps):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "#         if batch % 100 == 0:\n",
        "#             print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "#               epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "print ('Time taken for {} epoch {} sec\\n'.format(EPOCHS, time.time() - start))"
      ],
      "metadata": {
        "id": "agqVYAMcZ62n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ad3aa2-d940-4912-f631-efa1c61266a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [07:22<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 1.500475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:55<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss 0.684144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:55<00:00,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss 0.105266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:55<00:00,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss 0.052427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:55<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss 0.034074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:55<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss 0.024379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:55<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss 0.018488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:55<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss 0.015587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:56<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss 0.012971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:55<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Loss 0.010134\n",
            "Time taken for 10 epoch 4270.969752073288 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3mLLkl5BZ_jY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "74b93024-4968-4ec8-bbab-6a1eb067b1c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_str(result):\n",
        "  ans = ''\n",
        "  for i in result:\n",
        "    i = i.numpy()\n",
        "    if tokenizer.index_word[i] == '<end>':\n",
        "      break\n",
        "    elif tokenizer.index_word[i] == '<start>':\n",
        "      continue\n",
        "    ans += tokenizer.index_word[i]\n",
        "  return ans"
      ],
      "metadata": {
        "id": "-GHhLvAk2d4w"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "def evaluate(img_tensor):\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n",
        "\n",
        "    features = image_features_extract_model(img_tensor)\n",
        "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
        "    features = encoder(features)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    result = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "        predicted_id = tf.argmax(predictions ,axis=1).numpy()\n",
        "        dec_input = tf.expand_dims(predicted_id, 1)\n",
        "        result = tf.concat([result, predicted_id.reshape((BATCH_SIZE, 1))], axis=1)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "K-WL_3eHaBrx"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# captions on the validation set\n",
        "right_count = 0\n",
        "\n",
        "for (batch, (img_tensor, target)) in tqdm(enumerate(dataset_valid)):\n",
        "  result_batch = evaluate(img_tensor)\n",
        "  for id in range(BATCH_SIZE):\n",
        "    if get_str(result_batch[id]) == get_str(target[id]):\n",
        "      right_count += 1\n",
        "\n",
        "print(f'validation accuracy:{right_count/len(img_name_val)}')"
      ],
      "metadata": {
        "id": "hIfKxFz1aErR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0484b934-1e32-4f96-b0f9-af49c1358fda"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400it [03:31,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation accuracy:0.9487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_name = 'Lab12-2_110062209.txt'\n",
        "with open(output_name, 'w') as f:\n",
        "\n",
        "  for (batch, img_tensor) in tqdm(enumerate(dataset_test)):\n",
        "    result_batch = evaluate(img_tensor)\n",
        "    for id in range(BATCH_SIZE):\n",
        "      f.write(f'a{120000+id+batch*BATCH_SIZE} {get_str(result_batch[id])}')"
      ],
      "metadata": {
        "id": "-KXhwqOH027z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f1e485-0a70-44ea-f69e-ff64eff0892e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400it [02:32,  2.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "由於colab時間限制，我把epoch降到10，但10也夠把validation的accuracy提升到0.9487了"
      ],
      "metadata": {
        "id": "0oydeIQjkptU"
      }
    }
  ]
}