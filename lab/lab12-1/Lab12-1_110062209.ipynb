{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sd6DpjqxIgn-",
        "outputId": "7e3d8a3e-a879-4fd8-8804-3a1fde7feb96"
      },
      "outputs": [],
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.15.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbUJurrPsZV5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5ToXViIsvnO",
        "outputId": "810850fd-a430-44ef-ebc6-c7a4cfe8d8e3"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Restrict TensorFlow to only use the first GPU\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB-7hHGC2QOA",
        "outputId": "83f831f8-e288-4938-8caf-1f177bd0b5c5"
      },
      "outputs": [],
      "source": [
        "!gdown --id 18mmELoMQu0wMKp2Sku23T4fbC3q9Eqgl\n",
        "!unzip -q data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLmKZvSSsxQO"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "movie_reviews = pd.read_csv(\"./data/IMDB Dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01aRL0I9wCG8",
        "outputId": "4039ea01-ac45-4fde-cecd-260364646c12"
      },
      "outputs": [],
      "source": [
        "# check if there is any null value in the dataset\n",
        "movie_reviews.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqozyLHEwC9T",
        "outputId": "2e7d595f-773f-466a-e35f-7ff458526dfd"
      },
      "outputs": [],
      "source": [
        "# show the size of the dataset\n",
        "movie_reviews.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FLvhru5SwD4a",
        "outputId": "de3340c8-9005-41a3-9f80-aa3d28bcd972"
      },
      "outputs": [],
      "source": [
        "# show the first five data in the dataset\n",
        "movie_reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "xSPRY1QcwFpK",
        "outputId": "b14d82ce-c93d-4e36-cce9-d39e37d44e11"
      },
      "outputs": [],
      "source": [
        "movie_reviews[\"review\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKZppXYNvqVb"
      },
      "outputs": [],
      "source": [
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "\n",
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmXD470bvszU"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "sentences = list(movie_reviews['review'])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "\n",
        "# replace the positive with 1, replace the negative with 0\n",
        "y = movie_reviews['sentiment']\n",
        "y = np.array(list(map(lambda x: 1 if x == \"positive\" else 0, y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meDSxgmtvubz",
        "outputId": "735750b8-bbe3-4a27-9dea-9cdd4607543f"
      },
      "outputs": [],
      "source": [
        "# Split the training dataset and test dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "print(\"# training data: {:d}\\n# test data: {:d}\".format(len(X_train), len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIqJMJpYvwJL"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_len = 100\n",
        "# padding sentences to the same length\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPcL1PVuwIyz",
        "outputId": "190f4e3e-7526-435e-d2da-06595e881720"
      },
      "outputs": [],
      "source": [
        "# show the preprocessed data\n",
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1lbAPewKwq",
        "outputId": "53100df8-f8df-4908-e240-70c9a868ac58"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "# only reserve 10000 words\n",
        "vocab_size = 10000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRMQAf98wSwd"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        # vacab_size=10000, embedding_dim=256 enc_units=1024 batch_sz=64\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_activation='sigmoid',\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        # x is the training data with shape == (batch_sizeï¼Œmax_length)  -> (128, 100)\n",
        "        # which means there are batch_size sentences in one batch, the length of each sentence is max_length\n",
        "        # hidden state shape == (batch_size, units) -> (128, 1024)\n",
        "        # after embedding, x shape == (batch_size, max_length, embedding_dim) -> (128, 100, 256)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # output contains the state(in GRU, the hidden state and the output are same) from all timestamps,\n",
        "        # output shape == (batch_size, max_length, units) -> (128, 100, 1024)\n",
        "        # state is the hidden state of the last timestamp, shape == (batch_size, units) -> (128, 1024)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        # initialize the first state of the gru,  shape == (batch_size, units) -> (128, 1024)\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HleQMgjwUGF",
        "outputId": "aee7933a-0a2b-4799-f529-2a9b10263b38"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
        "# the output and the hidden state of GRU is equal\n",
        "print(sample_output[-1, -1, :] == sample_hidden[-1, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BIeToN6wY3z"
      },
      "outputs": [],
      "source": [
        "class LuongAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        # TODO: Complete the function.\n",
        "        self.W = tf.keras.layers.Dense(units)\n",
        "\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # TODO: Implement the Luong attention.\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = tf.matmul(self.W(values),tf.transpose(hidden_with_time_axis, perm=[0, 2, 1]))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Oj3hjOiwhF7"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "\n",
        "        # pass through four fully connected layers, the model will return\n",
        "        # the probability of the positivity of the sentence\n",
        "        self.fc_1 = tf.keras.layers.Dense(2048)\n",
        "        self.fc_2 = tf.keras.layers.Dense(512)\n",
        "        self.fc_3 = tf.keras.layers.Dense(64)\n",
        "        self.fc_4 = tf.keras.layers.Dense(1)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = LuongAttention(self.dec_units)\n",
        "\n",
        "    def call(self, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        output = self.fc_1(context_vector)\n",
        "        output = self.fc_2(output)\n",
        "        output = self.fc_3(output)\n",
        "        output = self.fc_4(output)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsRChaXhwjer",
        "outputId": "776e4077-2c9f-4994-fef1-70f2e1fdfeb4"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(units, BATCH_SIZE)\n",
        "sample_decoder_output, _ = decoder(sample_hidden, sample_output)\n",
        "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oha-6pDmwkeu"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    loss_ = loss_object(real, pred)\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhURvRj_wlqs"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './checkpoints/sentiment-analysis'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK7UGHCrwomj"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        # passing enc_output to the decoder\n",
        "        predictions, _ = decoder(enc_hidden, enc_output)\n",
        "\n",
        "        loss = loss_function(targ, predictions)\n",
        "\n",
        "    # collect all trainable variables\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # calculate the gradients for the whole variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    # apply the gradients on the variables\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm1F0QKKwtcb",
        "outputId": "c386fc40-6837-4c74-9c49-f418f0a5a8d2"
      },
      "outputs": [],
      "source": [
        "# set the epochs for training\n",
        "EPOCHS = 10\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # get the initial hidden state of gru\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqOsC8J-wu8j",
        "outputId": "b8ea1f1b-3b4b-44e9-e7ea-bcc51519c24e"
      },
      "outputs": [],
      "source": [
        "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0i8fyWuwv8U"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(inp, enc_hidden):\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        predictions, attention_weights = decoder(enc_hidden, enc_output)\n",
        "    return predictions, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfalhDV2wxxk"
      },
      "outputs": [],
      "source": [
        "def evaluate(test_data):\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "    for batch, (inp, targ) in enumerate(test_data):\n",
        "        if len(inp) != BATCH_SIZE:\n",
        "            enc_hidden = tf.zeros((len(inp), units))\n",
        "        # make prediction\n",
        "        if batch == 0:\n",
        "            predictions, attention_weights = test_step(inp, enc_hidden)\n",
        "            predictions, attention_weights = predictions.numpy(), attention_weights.numpy()\n",
        "        else:\n",
        "            _predictions, _attention_weights = test_step(inp, enc_hidden)\n",
        "            _predictions, _attention_weights = _predictions.numpy(), _attention_weights.numpy()\n",
        "            predictions = np.concatenate((predictions, _predictions))\n",
        "            attention_weights = np.concatenate((attention_weights, _attention_weights))\n",
        "\n",
        "    predictions = np.squeeze(predictions)\n",
        "    attention_weights = np.squeeze(attention_weights)\n",
        "    predictions[np.where(predictions < 0.5)] = 0\n",
        "    predictions[np.where(predictions >= 0.5)] = 1\n",
        "    return predictions, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeMCqbeIwzpj"
      },
      "outputs": [],
      "source": [
        "y_pred, attention_weights = evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F0cNTbbw0tD",
        "outputId": "372521a6-44e1-4c0d-ce5f-d6bc10ff21c0"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ', (y_pred == y_test).sum() / len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUR8HGa8w2IH",
        "outputId": "4aae398d-8d77-4277-8424-18512f81e900"
      },
      "outputs": [],
      "source": [
        "from termcolor import colored\n",
        "for idx, data in enumerate(X_test[:10]):\n",
        "    print('y_true: {:d}'.format(y_test[idx]))\n",
        "    print('y_predict: {:.0f}'.format(y_pred[idx]))\n",
        "\n",
        "    # get the twenty most largest attention weights\n",
        "    large_weights_idx = np.argsort(attention_weights[idx])[::-1][:10]\n",
        "\n",
        "    for _idx in range(len(data)):\n",
        "        word_idx = data[_idx]\n",
        "        if word_idx != 0:\n",
        "            if _idx in large_weights_idx:\n",
        "                #print(colored(tokenizer.index_word[word_idx], 'red'), end=' ')\n",
        "                # try this if termcolor is not working properly\n",
        "                print(f'\\033[31m{tokenizer.index_word[word_idx]}\\033[0m', end=' ')\n",
        "            else:\n",
        "                print(tokenizer.index_word[word_idx], end=' ')\n",
        "    print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
