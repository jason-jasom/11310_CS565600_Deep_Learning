{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEtzRWo-teSQ",
        "outputId": "18dac202-f32b-4c60-b297-cdd3a8b59feb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import moviepy.editor as mpy\n",
        "import skimage.transform\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow.keras.losses as kls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/ntasfi/PyGame-Learning-Environment\n",
        "!pip install -e ./PyGame-Learning-Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_7bjOb4uO1U",
        "outputId": "ac50ce6f-5948-4915-e3df-d08058027032"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        # Restrict TensorFlow to only use the fourth GPU\n",
        "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDwadSvs2L5b",
        "outputId": "e2bee173-271f-4960-d51b-35398a0c1707"
      },
      "outputs": [],
      "source": [
        "%cd ./PyGame-Learning-Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBPMd3sGuPVW",
        "outputId": "83d75daf-e488-4c46-ba1c-d7f41908640c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear\n",
        "from ple.games.flappybird import FlappyBird\n",
        "from ple import PLE\n",
        "\n",
        "game = FlappyBird()\n",
        "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
        "env.reset_game()\n",
        "\n",
        "test_game = FlappyBird()\n",
        "test_env = PLE(test_game, fps=30, display_screen=False)\n",
        "test_env.reset_game()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX-uiS_g2SLb",
        "outputId": "7abea0b5-1fdf-4469-f64e-e1515ec5f99a"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DexBbFlDuVW1"
      },
      "outputs": [],
      "source": [
        "path = './movie_f'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyIfAwoeuXP1"
      },
      "outputs": [],
      "source": [
        "hparas = {\n",
        "    'image_size': 84,\n",
        "    'num_stack': 4,\n",
        "    'action_dim': len(env.getActionSet()),\n",
        "    'hidden_size': 256,\n",
        "    'lr': 0.0001,\n",
        "    'gamma': 0.99,\n",
        "    'lambda': 0.95,\n",
        "    'clip_val': 0.2,\n",
        "    'ppo_epochs': 8,\n",
        "    'test_epochs': 1,\n",
        "    'num_steps': 512,\n",
        "    'mini_batch_size': 64,\n",
        "    'target_reward': 200,\n",
        "    'max_episode': 30000,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eukktn4ouynB"
      },
      "outputs": [],
      "source": [
        "# Please do not modify this method\n",
        "def make_anim(images, fps=60, true_image=False):\n",
        "    duration = len(images) / fps\n",
        "\n",
        "    def make_frame(t):\n",
        "        try:\n",
        "            x = images[int(len(images) / duration * t)]\n",
        "        except:\n",
        "            x = images[-1]\n",
        "\n",
        "        if true_image:\n",
        "            return x.astype(np.uint8)\n",
        "        else:\n",
        "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
        "\n",
        "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
        "    clip.fps = fps\n",
        "\n",
        "    return clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GClUXKyhu1zV"
      },
      "outputs": [],
      "source": [
        "def preprocess_screen(screen):\n",
        "    screen = skimage.transform.rotate(screen, -90, resize=True)\n",
        "    screen = screen[:400, :]\n",
        "    screen = skimage.transform.resize(screen, [hparas['image_size'], hparas['image_size'], 1])\n",
        "    return screen.astype(np.float32)\n",
        "\n",
        "def frames_to_state(input_frames):\n",
        "    if(len(input_frames) == 1):\n",
        "        state = np.concatenate(input_frames*4, axis=-1)\n",
        "    elif(len(input_frames) == 2):\n",
        "        state = np.concatenate(input_frames[0:1]*2 + input_frames[1:]*2, axis=-1)\n",
        "    elif(len(input_frames) == 3):\n",
        "        state = np.concatenate(input_frames + input_frames[2:], axis=-1)\n",
        "    else:\n",
        "        state = np.concatenate(input_frames[-4:], axis=-1)\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8Ra_65_u4mQ"
      },
      "outputs": [],
      "source": [
        "class ActorCriticNetwork(tf.keras.Model):\n",
        "    def __init__(self, hparas):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_extractor = tf.keras.Sequential([\n",
        "          # Convolutional Layers\n",
        "          tf.keras.layers.Conv2D(filters=32, kernel_size=8, strides=4),\n",
        "          tf.keras.layers.ReLU(),\n",
        "          tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2),\n",
        "          tf.keras.layers.ReLU(),\n",
        "          tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1),\n",
        "          tf.keras.layers.ReLU(),\n",
        "          # Embedding Layers\n",
        "          tf.keras.layers.Flatten(),\n",
        "          tf.keras.layers.Dense(hparas['hidden_size']),\n",
        "          tf.keras.layers.ReLU(),\n",
        "        ])\n",
        "\n",
        "        # Actor Network\n",
        "        self.actor = tf.keras.layers.Dense(hparas['action_dim'], activation='softmax')\n",
        "        # Critic Network\n",
        "        self.critic = tf.keras.layers.Dense(1, activation = None)\n",
        "\n",
        "    def call(self, input):\n",
        "        x = self.feature_extractor(input)\n",
        "        action_logits = self.actor(x)\n",
        "        value = self.critic(x)\n",
        "        return action_logits, value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNmCMsQsu--7"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, hparas):\n",
        "        self.gamma = hparas['gamma']\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=hparas['lr'])\n",
        "        self.actor_critic = ActorCriticNetwork(hparas)\n",
        "        self.clip_pram = hparas['clip_val']\n",
        "\n",
        "    def ppo_iter(self, mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "        batch_size = states.shape[0]\n",
        "        for _ in range(batch_size // mini_batch_size):\n",
        "            rand_ids = tf.convert_to_tensor(np.random.randint(0, batch_size, mini_batch_size), dtype=tf.int32)\n",
        "            yield tf.gather(states, rand_ids), tf.gather(actions, rand_ids), tf.gather(log_probs, rand_ids), \\\n",
        "             tf.gather(returns, rand_ids), tf.gather(advantage, rand_ids)\n",
        "\n",
        "    def ppo_update(self, ppo_epochs, mini_batch_size, states, actions, log_probs, discount_rewards, advantages):\n",
        "        total_actor_loss = 0\n",
        "        total_critic_loss = 0\n",
        "        for _ in range(ppo_epochs):\n",
        "            for state, action, old_log_probs, reward, advantage in self.ppo_iter(mini_batch_size, states, actions, log_probs, discount_rewards, advantages):\n",
        "                reward = tf.expand_dims(reward, axis=-1)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    prob, value = self.actor_critic(state, training=True)\n",
        "                    dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "                    entropy = tf.math.reduce_mean(dist.entropy())\n",
        "                    new_log_probs = dist.log_prob(action)\n",
        "\n",
        "                    # PPO ratio\n",
        "                    ratio = tf.math.exp(new_log_probs - old_log_probs)\n",
        "                    surr1 = ratio * advantage\n",
        "                    surr2 = tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram) * advantage\n",
        "\n",
        "                    actor_loss = tf.math.negative(tf.math.reduce_mean(tf.math.minimum(surr1, surr2))) - 0.1 * entropy\n",
        "                    critic_loss = 0.5 * tf.math.reduce_mean(kls.MeanSquaredError()(reward, value))\n",
        "\n",
        "                    total_loss = actor_loss + critic_loss\n",
        "\n",
        "                # single optimizer\n",
        "                grads = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
        "                self.optimizer.apply_gradients(zip(grads, self.actor_critic.trainable_variables))\n",
        "\n",
        "                total_actor_loss += actor_loss\n",
        "                total_critic_loss += critic_loss\n",
        "        return total_actor_loss, total_critic_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PPO:\n",
        "ratio = new_prob/old_prob\n",
        "\n",
        "advantage = $\\hat{A_{t}}$\n",
        "\n",
        "取min(ratio * advantage , tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram) * advantage)後變成負數加入loss以此最大化該值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AijstYOuvAkq"
      },
      "outputs": [],
      "source": [
        "# https://arxiv.org/pdf/1506.02438.pdf\n",
        "# Equation 16\n",
        "def compute_gae(rewards, masks, values, gamma, LAMBDA):\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for i in reversed(range(len(rewards))):\n",
        "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
        "        gae = delta + gamma * LAMBDA * masks[i] * gae\n",
        "        returns.append(gae + values[i])\n",
        "\n",
        "    returns.reverse()\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GAE:\n",
        "\n",
        "用每一輪累積的GAE * $\\gamma\\lambda$ + $r_{t}$ + $\\gamma$ V($s_{t+1}$) - V($s_{t}$)達成GAE = $\\sum_{l=0}^{\\infty}$ $(\\gamma\\lambda)^{l}$ $\\delta_{t+l}^V$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obY3CwOQvCC6"
      },
      "outputs": [],
      "source": [
        "def test_reward(test_env, agent):\n",
        "    total_reward = 0\n",
        "    # Reset the environment\n",
        "    test_env.reset_game()\n",
        "    input_frames = [preprocess_screen(test_env.getScreenGrayscale())]\n",
        "\n",
        "    while not test_env.game_over():\n",
        "\n",
        "        state = frames_to_state(input_frames)\n",
        "        state = tf.expand_dims(state, axis=0)\n",
        "        prob, value = agent.actor_critic(state)\n",
        "\n",
        "        action = np.argmax(prob[0].numpy())\n",
        "        reward = test_env.act(test_env.getActionSet()[action])\n",
        "        total_reward += reward\n",
        "\n",
        "        input_frames.append(preprocess_screen(test_env.getScreenGrayscale()))\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFHNbquLvDnZ"
      },
      "outputs": [],
      "source": [
        "agent = Agent(hparas)\n",
        "max_episode = hparas['max_episode']\n",
        "test_per_n_episode = 10\n",
        "force_save_per_n_episode = 1000\n",
        "early_stop_reward = 10\n",
        "\n",
        "start_s = 0\n",
        "best_reward = -5.0\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    actor_critic = agent.actor_critic,\n",
        "    optimizer = agent.optimizer,\n",
        ")\n",
        "\n",
        "# Load from old checkpoint\n",
        "# checkpoint.restore('ckpt_dir/ckpt-?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6pCQREw593c"
      },
      "outputs": [],
      "source": [
        "path = './save'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjXHtjDW6BNK"
      },
      "outputs": [],
      "source": [
        "path = './save/Actor'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cksMsBS6Fk5"
      },
      "outputs": [],
      "source": [
        "path = './save/checkpoints'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uVC6mKtmvLXI",
        "outputId": "cc4dc16e-0aa6-467d-f539-3632af67904f"
      },
      "outputs": [],
      "source": [
        "ep_reward = []\n",
        "total_avgr = []\n",
        "early_stop = False\n",
        "avg_rewards_list = []\n",
        "\n",
        "env.reset_game()\n",
        "\n",
        "for s in range(0, max_episode):\n",
        "    if early_stop == True:\n",
        "        break\n",
        "\n",
        "    rewards = []\n",
        "    states = []\n",
        "    actions = []\n",
        "    log_probs = []\n",
        "    masks = []\n",
        "    values = []\n",
        "\n",
        "    display_frames = [env.getScreenRGB()]\n",
        "    input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
        "\n",
        "    for step in range(hparas['num_steps']):\n",
        "\n",
        "        state = frames_to_state(input_frames)\n",
        "        state = tf.expand_dims(state, axis=0)\n",
        "        prob, value = agent.actor_critic(state)\n",
        "\n",
        "        dist = tfp.distributions.Categorical(probs=prob[0], dtype=tf.float32)\n",
        "        action = dist.sample(1)\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        reward = env.act(env.getActionSet()[int(action.numpy())])\n",
        "\n",
        "        done = env.game_over()\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        values.append(value[0])\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(tf.convert_to_tensor(reward, dtype=tf.float32))\n",
        "        masks.append(tf.convert_to_tensor(1-int(done), dtype=tf.float32))\n",
        "\n",
        "        display_frames.append(env.getScreenRGB())\n",
        "        input_frames.append(preprocess_screen(env.getScreenGrayscale()))\n",
        "\n",
        "        if done:\n",
        "            env.reset_game()\n",
        "            input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
        "\n",
        "    _, next_value = agent.actor_critic(state)\n",
        "    values.append(next_value[0])\n",
        "\n",
        "    returns = compute_gae(rewards, masks, values, hparas['gamma'], hparas['lambda'])\n",
        "\n",
        "    returns = tf.concat(returns, axis=0)\n",
        "    log_probs = tf.concat(log_probs, axis=0)\n",
        "    values = tf.concat(values, axis=0)\n",
        "    states = tf.concat(states, axis=0)\n",
        "    actions = tf.concat(actions, axis=0)\n",
        "    advantage = returns - values[:-1]\n",
        "\n",
        "    a_loss, c_loss = agent.ppo_update(hparas['ppo_epochs'], hparas['mini_batch_size'], states, actions, log_probs, returns, advantage)\n",
        "    print('[Episode %d]  Actor loss: %.5f, Critic loss: %.5f' % (s, a_loss, c_loss))\n",
        "\n",
        "    if s % test_per_n_episode == 0:\n",
        "        # test agent hparas['test_epochs'] times to get the average reward\n",
        "        avg_reward = np.mean([test_reward(test_env, agent) for _ in range(hparas['test_epochs'])])\n",
        "        print(\"Test average reward is %.1f, Current best average reward is %.1f\\n\" % (avg_reward, best_reward))\n",
        "        avg_rewards_list.append(avg_reward)\n",
        "\n",
        "        if avg_reward > best_reward:\n",
        "            best_reward = avg_reward\n",
        "            agent.actor_critic.save('./save/Actor/model_actor_{}_{}.keras'.format(s, avg_reward))\n",
        "            checkpoint.save(file_prefix = './save/checkpoints/ckpt')\n",
        "\n",
        "    if s % force_save_per_n_episode == 0:\n",
        "        agent.actor_critic.save('./save/Actor/model_actor_{}_{}.keras'.format(s, avg_reward))\n",
        "        checkpoint.save(file_prefix = './save/checkpoints/ckpt')\n",
        "        clip = make_anim(display_frames, fps=60, true_image=True).rotate(-90)\n",
        "        clip.write_videofile(\"movie_f/{}_demo-{}.webm\".format('Lab15', s), fps=60)\n",
        "        display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))\n",
        "\n",
        "    if best_reward >= early_stop_reward:\n",
        "        early_stop = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(range(len(avg_rewards_list)), avg_rewards_list)\n",
        "plt.title('Average Reward')\n",
        "plt.xlabel('episode (every 10 episode)')\n",
        "plt.ylabel('average reward')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Report**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "結合GAE(returns計算)與PPO(update策略)\n",
        "\n",
        "在1000 episode 後就能勉強過1根管子，在1560 episode時best average reward開始>0，在3000 episode後就能高機率過1根管子並繼續前進。\n",
        "\n",
        "到4020 episode後average reward>10並停止。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
